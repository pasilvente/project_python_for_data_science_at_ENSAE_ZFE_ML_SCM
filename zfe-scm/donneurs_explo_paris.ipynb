{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd76429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- chemins ----------\n",
    "HERE = Path().resolve()      # zfe-scm/\n",
    "ROOT = HERE.parent           # racine du projet\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# üîÅ nom du fichier brut (tel que dans ton dossier data)\n",
    "RAW_FILE = \"Export Max. journalier moy. hor. - 20251226130804 - 2017-08-17 00_00 - 2025-04-12 00_00.csv\"\n",
    "\n",
    "raw_path = DATA / RAW_FILE\n",
    "\n",
    "# ---------- 1) chargement & nettoyage du fichier donneurs ----------\n",
    "df_raw = pd.read_csv(raw_path, sep=\";\", engine=\"python\")\n",
    "\n",
    "# au cas o√π il y aurait plusieurs polluants, on s√©curise\n",
    "df_no2 = df_raw[df_raw[\"Polluant\"] == \"NO2\"].copy()\n",
    "\n",
    "# date + coordonn√©es\n",
    "df_no2[\"date\"] = pd.to_datetime(df_no2[\"Date de d√©but\"])\n",
    "df_no2[\"lat\"] = df_no2[\"Latitude\"].astype(float)\n",
    "df_no2[\"lon\"] = df_no2[\"Longitude\"].astype(float)\n",
    "\n",
    "# mettre au format commun\n",
    "new_donors = (\n",
    "    df_no2.rename(columns={\n",
    "        \"code site\": \"station_id\",\n",
    "        \"nom site\": \"station_name\",\n",
    "        \"type d'implantation\": \"station_env\",\n",
    "        \"type d'influence\": \"station_influence\",\n",
    "        \"valeur\": \"no2_ug_m3\",\n",
    "    })[\n",
    "        [\"date\",\n",
    "         \"station_id\", \"station_name\",\n",
    "         \"station_env\", \"station_influence\",\n",
    "         \"no2_ug_m3\", \"lat\", \"lon\"]\n",
    "    ]\n",
    "    .sort_values([\"station_id\", \"date\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Aper√ßu des nouvelles stations donneuses :\")\n",
    "display(new_donors[[\"station_id\", \"station_name\",\n",
    "                    \"station_env\", \"station_influence\"]].drop_duplicates())\n",
    "\n",
    "# ---------- 2) sauvegarde standalone des donneurs ----------\n",
    "donors_daily_path = DATA / \"no2_donors_france_daily_clean.csv\"\n",
    "new_donors.to_csv(donors_daily_path, index=False)\n",
    "print(f\"‚úÖ Donneurs nettoy√©s enregistr√©s dans : {donors_daily_path.name}\")\n",
    "\n",
    "# ---------- 3) mise √† jour du gros fichier global des donneurs ----------\n",
    "all_daily_path = DATA / \"no2_all_stations_daily_clean.csv\"\n",
    "\n",
    "if all_daily_path.exists():\n",
    "    all_daily = pd.read_csv(all_daily_path)\n",
    "    all_daily[\"station_id\"] = all_daily[\"station_id\"].astype(str).str.strip()\n",
    "else:\n",
    "    all_daily = pd.DataFrame(columns=new_donors.columns)\n",
    "\n",
    "new_donors[\"station_id\"] = new_donors[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "combined = (\n",
    "    pd.concat([all_daily, new_donors], ignore_index=True)\n",
    "    .drop_duplicates(subset=[\"date\", \"station_id\"])\n",
    "    .sort_values([\"station_id\", \"date\"])\n",
    ")\n",
    "\n",
    "combined.to_csv(all_daily_path, index=False)\n",
    "print(f\"‚úÖ Fichier global des donneurs mis √† jour : {all_daily_path.name}\")\n",
    "\n",
    "# ---------- 4) mise √† jour des m√©tadonn√©es de stations ----------\n",
    "donors_meta_new = (\n",
    "    new_donors\n",
    "    .groupby([\"station_id\", \"station_name\",\n",
    "              \"station_env\", \"station_influence\"])[[\"lat\", \"lon\"]]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "meta_path = DATA / \"no2_stations_meta.csv\"\n",
    "\n",
    "if meta_path.exists():\n",
    "    meta_old = pd.read_csv(meta_path)\n",
    "else:\n",
    "    meta_old = pd.DataFrame(columns=donors_meta_new.columns)\n",
    "\n",
    "meta_all = (\n",
    "    pd.concat([meta_old, donors_meta_new], ignore_index=True)\n",
    "    .drop_duplicates(subset=[\"station_id\"], keep=\"first\")\n",
    ")\n",
    "\n",
    "meta_all.to_csv(meta_path, index=False)\n",
    "print(f\"‚úÖ M√©tadonn√©es stations mises √† jour : {meta_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d1e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- chemins ----------\n",
    "HERE = Path().resolve()      # zfe-scm/\n",
    "ROOT = HERE.parent           # racine du projet\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# ---------- 1) charger les donneurs ----------\n",
    "donors = pd.read_csv(DATA / \"no2_donors_france_daily_clean.csv\")\n",
    "donors[\"date\"] = pd.to_datetime(donors[\"date\"])\n",
    "\n",
    "print(\"Stations donneuses pr√©sentes :\")\n",
    "display(\n",
    "    donors[[\"station_id\", \"station_name\", \"station_env\", \"station_influence\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"station_name\")\n",
    ")\n",
    "\n",
    "# ---------- 2) agr√©gation mensuelle ----------\n",
    "monthly = (\n",
    "    donors\n",
    "    .set_index(\"date\")\n",
    "    .groupby([\"station_id\", \"station_name\"])[\"no2_ug_m3\"]\n",
    "    .resample(\"MS\")          # MS = d√©but de mois\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Aper√ßu du panel mensuel donneurs :\")\n",
    "display(monthly.head())\n",
    "\n",
    "# ---------- 3) trac√© ----------\n",
    "plt.figure(figsize=(11, 6))\n",
    "\n",
    "for (sid, name), sub in monthly.groupby([\"station_id\", \"station_name\"]):\n",
    "    plt.plot(sub[\"date\"], sub[\"no2_ug_m3\"], label=f\"{name} ({sid})\")\n",
    "\n",
    "plt.title(\"NO‚ÇÇ mensuel ‚Äì stations donneuses (France)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NO‚ÇÇ (¬µg/m¬≥)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad1d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- chemins ----------\n",
    "HERE = Path().resolve()      # zfe-scm/\n",
    "ROOT = HERE.parent           # racine du projet\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "donors_path = DATA / \"no2_donors_france_daily_clean.csv\"\n",
    "\n",
    "donors = pd.read_csv(donors_path)\n",
    "donors[\"date\"] = pd.to_datetime(donors[\"date\"])\n",
    "donors[\"station_id\"] = donors[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Stations donneuses :\")\n",
    "display(\n",
    "    donors[[\"station_id\", \"station_name\", \"station_env\", \"station_influence\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"station_name\")\n",
    ")\n",
    "\n",
    "# ---------- 1) d√©tecter les valeurs manquantes par station ----------\n",
    "gaps_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "for sid, sub in donors.groupby(\"station_id\"):\n",
    "    name = sub[\"station_name\"].iloc[0]\n",
    "    \n",
    "    sub = sub.sort_values(\"date\").set_index(\"date\")\n",
    "    \n",
    "    # recr√©er un index quotidien complet\n",
    "    full_idx = pd.date_range(start=sub.index.min(), end=sub.index.max(), freq=\"D\")\n",
    "    sub_full = sub.reindex(full_idx)\n",
    "    \n",
    "    # True si valeur manquante (NaN ou jour absent)\n",
    "    is_missing = sub_full[\"no2_ug_m3\"].isna()\n",
    "    total_missing = int(is_missing.sum())\n",
    "    \n",
    "    summary_rows.append({\n",
    "        \"station_id\": sid,\n",
    "        \"station_name\": name,\n",
    "        \"total_jours_manquants\": total_missing,\n",
    "        \"date_min\": sub_full.index.min(),\n",
    "        \"date_max\": sub_full.index.max(),\n",
    "    })\n",
    "    \n",
    "    if not is_missing.any():\n",
    "        continue  # pas de trous pour cette station\n",
    "    \n",
    "    # trouver les intervalles cons√©cutifs de True\n",
    "    shifted_prev = is_missing.shift(1, fill_value=False)\n",
    "    shifted_next = is_missing.shift(-1, fill_value=False)\n",
    "    \n",
    "    start_mask = is_missing & ~shifted_prev\n",
    "    end_mask = is_missing & ~shifted_next\n",
    "    \n",
    "    starts = sub_full.index[start_mask]\n",
    "    ends = sub_full.index[end_mask]\n",
    "    \n",
    "    for start, end in zip(starts, ends):\n",
    "        length = (end - start).days + 1\n",
    "        gaps_rows.append({\n",
    "            \"station_id\": sid,\n",
    "            \"station_name\": name,\n",
    "            \"gap_start\": start,\n",
    "            \"gap_end\": end,\n",
    "            \"gap_length_days\": length,\n",
    "        })\n",
    "\n",
    "# ---------- 2) tableaux r√©cap ----------\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"total_jours_manquants\", ascending=False)\n",
    "gaps_df = pd.DataFrame(gaps_rows).sort_values([\"station_id\", \"gap_start\"])\n",
    "\n",
    "print(\"R√©sum√© des jours manquants par station :\")\n",
    "display(summary_df)\n",
    "\n",
    "print(\"Intervalles de donn√©es manquantes (tous donneurs confondus) :\")\n",
    "display(gaps_df.head(50))  # tu peux enlever head(50) si tu veux tout voir\n",
    "\n",
    "# Si tu veux les sauvegarder en CSV :\n",
    "summary_df.to_csv(DATA / \"no2_donors_missing_summary.csv\", index=False)\n",
    "gaps_df.to_csv(DATA / \"no2_donors_missing_gaps.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Sauvegard√© :\")\n",
    "print(\" - no2_donors_missing_summary.csv\")\n",
    "print(\" - no2_donors_missing_gaps.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- chemins ----------\n",
    "HERE = Path().resolve()      # zfe-scm/\n",
    "ROOT = HERE.parent\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "donors_daily = pd.read_csv(DATA / \"no2_donors_france_daily_clean.csv\")\n",
    "donors_daily[\"date\"] = pd.to_datetime(donors_daily[\"date\"])\n",
    "donors_daily[\"station_id\"] = donors_daily[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "# garder la m√©ta (nom de station, type, coords)\n",
    "meta = donors_daily[[\n",
    "    \"station_id\", \"station_name\",\n",
    "    \"station_env\", \"station_influence\",\n",
    "    \"lat\", \"lon\"\n",
    "]].drop_duplicates()\n",
    "\n",
    "# ---------- 1) agr√©gation mensuelle (mean + nb de jours dispo) ----------\n",
    "monthly = (\n",
    "    donors_daily\n",
    "    .set_index(\"date\")\n",
    "    .groupby(\"station_id\")[\"no2_ug_m3\"]\n",
    "    .resample(\"MS\")              # d√©but de mois\n",
    "    .agg([\"mean\", \"count\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# pivot large\n",
    "mean_wide = monthly.pivot(index=\"date\", columns=\"station_id\", values=\"mean\")\n",
    "count_wide = monthly.pivot(index=\"date\", columns=\"station_id\", values=\"count\")\n",
    "\n",
    "# ---------- 2) on impose un minimum de jours dans le mois ----------\n",
    "min_days = 10  # tu peux ajuster √† 5, 15, etc.\n",
    "mean_wide[count_wide < min_days] = np.nan\n",
    "\n",
    "print(\"Nb de mois manquants par station AVANT imputation :\")\n",
    "missing_before = mean_wide.isna().sum()\n",
    "display(missing_before.to_frame(\"nb_mois_na\").sort_values(\"nb_mois_na\", ascending=False))\n",
    "\n",
    "# ---------- 3) interpolation temporelle par station ----------\n",
    "mean_interp = mean_wide.interpolate(limit_direction=\"both\")\n",
    "\n",
    "print(\"Nb de mois manquants par station APRES imputation :\")\n",
    "missing_after = mean_interp.isna().sum()\n",
    "display(missing_after.to_frame(\"nb_mois_na\").sort_values(\"nb_mois_na\", ascending=False))\n",
    "\n",
    "# (optionnel) virer les stations vraiment trop pourries\n",
    "max_missing_allowed = 12  # ex : tu acceptes au plus 12 mois manquants sur toute la p√©riode\n",
    "good_stations = missing_before[missing_before <= max_missing_allowed].index.tolist()\n",
    "mean_interp = mean_interp[good_stations]\n",
    "\n",
    "print(\"Stations conserv√©es apr√®s filtrage :\", good_stations)\n",
    "\n",
    "# ---------- 4) repasser en long + sauvegarde ----------\n",
    "monthly_long = (\n",
    "    mean_interp\n",
    "    .reset_index()\n",
    "    .melt(id_vars=\"date\", var_name=\"station_id\", value_name=\"no2_ug_m3\")\n",
    "    .dropna(subset=[\"no2_ug_m3\"])\n",
    ")\n",
    "\n",
    "# join avec la m√©ta\n",
    "monthly_long = monthly_long.merge(meta, on=\"station_id\", how=\"left\")\n",
    "\n",
    "monthly_path = DATA / \"no2_donors_france_monthly_imputed.csv\"\n",
    "monthly_long.to_csv(monthly_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Fichier donneurs mensuel imput√© enregistr√© dans :\", monthly_path.name)\n",
    "display(monthly_long.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac84f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# =============================\n",
    "# 0. Chemins & chargement\n",
    "# =============================\n",
    "HERE = Path().resolve()      # dossier zfe-scm\n",
    "ROOT = HERE.parent\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# Paris (toutes stations)\n",
    "paris_daily = pd.read_csv(DATA / \"pollution_paris_no2_daily_clean.csv\")\n",
    "paris_daily[\"date\"] = pd.to_datetime(paris_daily[\"date\"])\n",
    "paris_daily[\"station_id\"] = paris_daily[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Stations Paris :\")\n",
    "display(paris_daily[[\"station_id\", \"station_name\"]].drop_duplicates())\n",
    "\n",
    "# Donneurs mensuels imput√©s\n",
    "donors_monthly = pd.read_csv(DATA / \"no2_donors_france_monthly_imputed.csv\")\n",
    "donors_monthly[\"date\"] = pd.to_datetime(donors_monthly[\"date\"])\n",
    "donors_monthly[\"station_id\"] = donors_monthly[\"station_id\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Stations donneuses utilis√©es :\")\n",
    "display(\n",
    "    donors_monthly[[\"station_id\", \"station_name\", \"station_env\", \"station_influence\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"station_name\")\n",
    ")\n",
    "\n",
    "# Meta ZFE pour la date de traitement Paris\n",
    "zfe_meta = pd.read_csv(DATA / \"zfe_meta.csv\")\n",
    "paris_row = zfe_meta.loc[zfe_meta[\"publisher_zfe_id\"] == \"PARIS\"].iloc[0]\n",
    "zfe_start = pd.to_datetime(paris_row[\"first_date_debut\"])\n",
    "zfe_start_month = zfe_start.to_period(\"M\").to_timestamp()\n",
    "print(\"D√©but ZFE Paris (d'apr√®s zfe_meta) :\", zfe_start.date())\n",
    "\n",
    "# =============================\n",
    "# 1. Station trait√©e = Champs-√âlys√©es\n",
    "# =============================\n",
    "\n",
    "treated_df = paris_daily[\n",
    "    paris_daily[\"station_name\"].str.lower().str.contains(\"champs\", na=False)\n",
    "].copy()\n",
    "\n",
    "if treated_df.empty:\n",
    "    treated_df = paris_daily[\n",
    "        paris_daily[\"station_name\"].str.lower().str.contains(\"elys\", na=False)\n",
    "    ].copy()\n",
    "\n",
    "if treated_df.empty:\n",
    "    raise ValueError(\"Impossible de trouver la station Champs-√âlys√©es dans pollution_paris_no2_daily_clean.csv\")\n",
    "\n",
    "treated_id = treated_df[\"station_id\"].iloc[0]\n",
    "treated_name = treated_df[\"station_name\"].iloc[0]\n",
    "\n",
    "print(f\"Station trait√©e : {treated_name} ({treated_id})\")\n",
    "\n",
    "treated_monthly = (\n",
    "    treated_df\n",
    "    .set_index(\"date\")[\"no2_ug_m3\"]\n",
    "    .resample(\"MS\")\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# 2. Panel mensuel donneurs (on s√©curise les doublons)\n",
    "# =============================\n",
    "\n",
    "donors_monthly_agg = (\n",
    "    donors_monthly\n",
    "    .groupby([\"date\", \"station_id\"], as_index=False)[\"no2_ug_m3\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "donors_wide = (\n",
    "    donors_monthly_agg\n",
    "    .pivot(index=\"date\", columns=\"station_id\", values=\"no2_ug_m3\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# 3. Construction du panel (trait√©e + donneurs)\n",
    "# =============================\n",
    "\n",
    "panel = donors_wide.copy()\n",
    "panel[\"treated\"] = treated_monthly\n",
    "panel = panel.dropna(subset=[\"treated\"])\n",
    "\n",
    "start = pd.to_datetime(\"2017-08-01\")\n",
    "end   = pd.to_datetime(\"2024-12-01\")\n",
    "panel = panel.loc[(panel.index >= start) & (panel.index <= end)]\n",
    "\n",
    "donor_matrix = panel.drop(columns=[\"treated\"]).copy()\n",
    "donor_matrix = donor_matrix.interpolate(limit_direction=\"both\")\n",
    "donor_matrix = donor_matrix.dropna(axis=1, how=\"all\")\n",
    "\n",
    "donor_ids = donor_matrix.columns.tolist()\n",
    "treated_series = panel[\"treated\"]\n",
    "\n",
    "print(\"P√©riode du panel :\", treated_series.index.min().date(), \"‚Üí\", treated_series.index.max().date())\n",
    "print(\"Nombre de donneurs utilis√©s :\", len(donor_ids))\n",
    "\n",
    "pre_mask = treated_series.index < zfe_start_month\n",
    "post_mask = treated_series.index >= zfe_start_month\n",
    "\n",
    "X_pre = donor_matrix.loc[pre_mask]\n",
    "y_pre = treated_series.loc[pre_mask]\n",
    "X_all = donor_matrix\n",
    "\n",
    "# =============================\n",
    "# 4. Fonctions SCM\n",
    "# =============================\n",
    "\n",
    "def fit_ridge(X_pre, y_pre, X_all):\n",
    "    alphas = np.logspace(-3, 3, 50)\n",
    "    model = RidgeCV(alphas=alphas, cv=5, fit_intercept=False)\n",
    "    model.fit(X_pre, y_pre)\n",
    "    y_hat = pd.Series(model.predict(X_all), index=X_all.index, name=\"ridge\")\n",
    "    weights = pd.Series(model.coef_, index=X_pre.columns, name=\"ridge\")\n",
    "    return y_hat, weights, model.alpha_\n",
    "\n",
    "def fit_lasso(X_pre, y_pre, X_all):\n",
    "    alphas = np.logspace(-3, 1, 50)\n",
    "    model = LassoCV(alphas=alphas, cv=5, fit_intercept=False, max_iter=10000)\n",
    "    model.fit(X_pre, y_pre)\n",
    "    y_hat = pd.Series(model.predict(X_all), index=X_all.index, name=\"lasso\")\n",
    "    weights = pd.Series(model.coef_, index=X_pre.columns, name=\"lasso\")\n",
    "    return y_hat, weights, model.alpha_\n",
    "\n",
    "def fit_elasticnet(X_pre, y_pre, X_all):\n",
    "    alphas = np.logspace(-3, 1, 40)\n",
    "    l1s = [0.1, 0.5, 0.9]\n",
    "    model = ElasticNetCV(\n",
    "        alphas=alphas,\n",
    "        l1_ratio=l1s,\n",
    "        cv=5,\n",
    "        fit_intercept=False,\n",
    "        max_iter=10000,\n",
    "    )\n",
    "    model.fit(X_pre, y_pre)\n",
    "    y_hat = pd.Series(model.predict(X_all), index=X_all.index, name=\"elasticnet\")\n",
    "    weights = pd.Series(model.coef_, index=X_all.columns, name=\"elasticnet\")\n",
    "    return y_hat, weights, (model.alpha_, model.l1_ratio_)\n",
    "\n",
    "# =============================\n",
    "# 5. Ajustements Ridge / Lasso / ElasticNet\n",
    "# =============================\n",
    "\n",
    "y_ridge, w_ridge, alpha_ridge = fit_ridge(X_pre, y_pre, X_all)\n",
    "y_lasso, w_lasso, alpha_lasso = fit_lasso(X_pre, y_pre, X_all)\n",
    "y_en, w_en, (alpha_en, l1_en) = fit_elasticnet(X_pre, y_pre, X_all)\n",
    "\n",
    "print(f\"Ridge ‚Äì alpha* = {alpha_ridge:.4f}\")\n",
    "print(f\"Lasso ‚Äì alpha* = {alpha_lasso:.4f}\")\n",
    "print(f\"ElasticNet ‚Äì alpha* = {alpha_en:.4f}, l1_ratio* = {l1_en:.2f}\")\n",
    "\n",
    "for name, y_syn in [(\"ridge\", y_ridge), (\"lasso\", y_lasso), (\"elasticnet\", y_en)]:\n",
    "    mse_pre = mean_squared_error(\n",
    "        treated_series[pre_mask],\n",
    "        y_syn[pre_mask],\n",
    "    )\n",
    "    rmse_pre = mse_pre ** 0.5\n",
    "    print(f\"RMSE pr√©-ZFE ({name}) : {rmse_pre:.3f}\")\n",
    "\n",
    "# =============================\n",
    "# 6. Graphique s√©rie temporelle\n",
    "# =============================\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(treated_series.index, treated_series.values,\n",
    "         label=f\"{treated_name} ({treated_id}) observ√©\", linewidth=2)\n",
    "\n",
    "plt.plot(y_ridge.index, y_ridge.values, label=\"SCM Ridge\", linestyle=\"--\")\n",
    "plt.plot(y_lasso.index, y_lasso.values, label=\"SCM Lasso\", linestyle=\"--\")\n",
    "plt.plot(y_en.index, y_en.values, label=\"SCM ElasticNet\", linestyle=\"--\")\n",
    "\n",
    "plt.axvline(zfe_start_month, color=\"red\", linestyle=\"--\", linewidth=2,\n",
    "            label=f\"D√©but ZFE Paris ({zfe_start_month.date()})\")\n",
    "\n",
    "plt.title(\"NO‚ÇÇ mensuel ‚Äì Paris Champs-√âlys√©es vs contr√¥les synth√©tiques\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NO‚ÇÇ (¬µg/m¬≥)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================\n",
    "# 7. Graphique des poids\n",
    "# =============================\n",
    "\n",
    "weights_df = pd.concat([w_ridge, w_lasso, w_en], axis=1).fillna(0.0)\n",
    "weights_df.index.name = \"station_id\"\n",
    "\n",
    "weights_df[\"mean_abs\"] = weights_df.abs().mean(axis=1)\n",
    "weights_df = weights_df.sort_values(\"mean_abs\", ascending=False)\n",
    "weights_df = weights_df.drop(columns=[\"mean_abs\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "weights_df.plot(kind=\"bar\")\n",
    "plt.title(\"Poids des donneurs ‚Äì Ridge / Lasso / ElasticNet (Champs-√âlys√©es)\")\n",
    "plt.xlabel(\"Station donneuse\")\n",
    "plt.ylabel(\"Poids (coefficients)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "weights_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a705bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================\n",
    "# ATT SCM ‚Äì Paris Champs-√âlys√©es\n",
    "# =============================\n",
    "\n",
    "# 1) Construire les s√©ries ATT_t = obs - synth\n",
    "synth_dict = {\n",
    "    \"Ridge\": y_ridge,\n",
    "    \"Lasso\": y_lasso,\n",
    "    \"ElasticNet\": y_en,\n",
    "}\n",
    "\n",
    "att_dict = {}\n",
    "for name, y_hat in synth_dict.items():\n",
    "    # aligner proprement les dates\n",
    "    common_idx = treated_series.index.intersection(y_hat.index)\n",
    "    att = treated_series.loc[common_idx] - y_hat.loc[common_idx]\n",
    "    att.name = f\"ATT_{name}\"\n",
    "    att_dict[name] = att\n",
    "\n",
    "# 2) Masques temporels (global)\n",
    "dates = treated_series.index\n",
    "covid_start = pd.to_datetime(\"2020-03-01\")\n",
    "covid_end   = pd.to_datetime(\"2021-06-01\")\n",
    "\n",
    "# 3) Tableau r√©capitulatif des ATT moyens\n",
    "rows = []\n",
    "for name, att in att_dict.items():\n",
    "    idx = att.index\n",
    "    pre = idx < zfe_start_month\n",
    "    post = idx >= zfe_start_month\n",
    "    covid = (idx >= covid_start) & (idx <= covid_end)\n",
    "    post_nocovid = post & ~covid\n",
    "\n",
    "    rows.append({\n",
    "        \"m√©thode\": name,\n",
    "        \"ATT_moy_pre\": att[pre].mean(),\n",
    "        \"ATT_moy_post\": att[post].mean(),\n",
    "        \"ATT_moy_post_sans_Covid\": att[post_nocovid].mean(),\n",
    "    })\n",
    "\n",
    "att_summary = pd.DataFrame(rows)\n",
    "print(\"R√©sum√© des ATT (en ¬µg/m¬≥) ‚Äì Paris Champs-√âlys√©es :\")\n",
    "display(att_summary)\n",
    "\n",
    "# 4) Graphique des ATT dans le temps\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for name, att in att_dict.items():\n",
    "    plt.plot(att.index, att.values, label=f\"ATT {name}\")\n",
    "\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.axvline(zfe_start_month, color=\"red\", linestyle=\"--\", linewidth=2,\n",
    "            label=f\"D√©but ZFE Paris ({zfe_start_month.date()})\")\n",
    "\n",
    "# zone Covid en gris\n",
    "plt.axvspan(covid_start, covid_end, color=\"grey\", alpha=0.2, label=\"P√©riode Covid approx.\")\n",
    "\n",
    "plt.title(\"ATT mensuel (NO‚ÇÇ observ√© - synth√©tique) ‚Äì Paris Champs-√âlys√©es\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ATT (¬µg/m¬≥)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
