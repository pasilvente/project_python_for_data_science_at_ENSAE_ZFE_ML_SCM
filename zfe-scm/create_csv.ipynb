{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1999e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# On part du dossier où est le notebook: PROJET/.../zfe-scm\n",
    "HERE = Path().resolve()\n",
    "print(\"cwd =\", HERE)\n",
    "\n",
    "# Le dossier racine du projet = parent de zfe-scm\n",
    "PROJECT_ROOT = HERE.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "print(\"data dir =\", DATA_DIR)\n",
    "\n",
    "# --- chemins complets ---\n",
    "aires_path = DATA_DIR / \"aires.geojson\"\n",
    "voies_path = DATA_DIR / \"voies.geojson\"\n",
    "zfe_ids_path = DATA_DIR / \"zfe_ids.csv\"\n",
    "\n",
    "print(aires_path.exists(), voies_path.exists(), zfe_ids_path.exists())\n",
    "\n",
    "# --- charger les fichiers ---\n",
    "with aires_path.open(encoding=\"utf-8\") as f:\n",
    "    aires_gj = json.load(f)\n",
    "\n",
    "with voies_path.open(encoding=\"utf-8\") as f:\n",
    "    voies_gj = json.load(f)\n",
    "\n",
    "zfe_ids = pd.read_csv(zfe_ids_path, sep=\";\")\n",
    "\n",
    "print(\"aires keys:\", aires_gj.keys())\n",
    "print(\"voies keys:\", voies_gj.keys())\n",
    "print(\"zfe_ids shape:\", zfe_ids.shape)\n",
    "zfe_ids.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ca8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_geojson(gj):\n",
    "    rows = []\n",
    "    for feat in gj[\"features\"]:\n",
    "        props = feat.get(\"properties\", {}).copy()\n",
    "        pub = feat.get(\"publisher\", {})\n",
    "        for k, v in pub.items():\n",
    "            props[f\"publisher_{k}\"] = v\n",
    "        rows.append(props)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "aires_df = flatten_geojson(aires_gj)\n",
    "voies_df = flatten_geojson(voies_gj)\n",
    "\n",
    "aires_df.head(), voies_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6023e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aires_df.to_csv(DATA_DIR / \"aires_flat.csv\", index=False)\n",
    "voies_df.to_csv(DATA_DIR / \"voies_flat.csv\", index=False)\n",
    "\n",
    "print(\"aires_flat :\", aires_df.shape)\n",
    "print(\"voies_flat :\", voies_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path().resolve().parent   # on est dans zfe-scm, on remonte\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "aires = pd.read_csv(DATA / \"aires_flat.csv\")\n",
    "voies = pd.read_csv(DATA / \"voies_flat.csv\")\n",
    "zfe_ids = pd.read_csv(DATA / \"zfe_ids.csv\", sep=\";\")\n",
    "\n",
    "print(\"aires:\", aires.shape, \"voies:\", voies.shape)\n",
    "\n",
    "# 1) Colonnes qu'on garde\n",
    "aires_keep = [\n",
    "    \"publisher_zfe_id\",\n",
    "    \"publisher_nom\",\n",
    "    \"publisher_siren\",\n",
    "    \"publisher_forme_juridique\",\n",
    "    \"id\",\n",
    "    \"date_debut\",\n",
    "    \"date_fin\",\n",
    "    \"vp_critair\", \"vp_horaires\",\n",
    "    \"vul_critair\", \"vul_horaires\",\n",
    "    \"pl_critair\", \"pl_horaires\",\n",
    "    \"autobus_autocars_critair\", \"autobus_autocars_horaires\",\n",
    "    \"deux_rm_critair\", \"deux_rm_horaires\",\n",
    "    \"url_arrete\",\n",
    "    \"url_site_information\",\n",
    "]\n",
    "\n",
    "voies_keep = [\n",
    "    \"publisher_zfe_id\",\n",
    "    \"publisher_nom\",\n",
    "    \"publisher_siren\",\n",
    "    \"publisher_forme_juridique\",\n",
    "    \"id\",\n",
    "    \"osm_id\",\n",
    "    \"ref\",\n",
    "    \"one_way\",\n",
    "    \"date_debut\",\n",
    "    \"date_fin\",\n",
    "    \"vp_critair\", \"vp_horaires\",\n",
    "    \"vul_critair\", \"vul_horaires\",\n",
    "    \"pl_critair\", \"pl_horaires\",\n",
    "    \"autobus_autocars_critair\", \"autobus_autocars_horaires\",\n",
    "    \"deux_rm_critair\", \"deux_rm_horaires\",\n",
    "    \"zfe_derogation\",\n",
    "    \"url_arrete\",\n",
    "    \"url_site\",\n",
    "    \"url_site_information\",\n",
    "]\n",
    "\n",
    "aires_clean = aires[aires_keep].copy()\n",
    "voies_clean = voies[voies_keep].copy()\n",
    "\n",
    "# 2) Convertir dates en datetime\n",
    "for df in (aires_clean, voies_clean):\n",
    "    for col in [\"date_debut\", \"date_fin\"]:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "# 3) Table méta ZFE (1 ligne par ZFE)\n",
    "zfe_meta = (\n",
    "    aires_clean\n",
    "    .groupby([\"publisher_zfe_id\", \"publisher_siren\", \"publisher_nom\"], as_index=False)\n",
    "    .agg(\n",
    "        first_date_debut=(\"date_debut\", \"min\"),\n",
    "        last_date_debut=(\"date_debut\", \"max\"),\n",
    "        first_date_fin=(\"date_fin\", lambda s: s.dropna().min() if s.notna().any() else pd.NaT),\n",
    "        n_aires=(\"id\", \"nunique\"),\n",
    "        has_vp_restriction=(\"vp_critair\", lambda s: s.notna().any())\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4) Jointure avec zfe_ids pour récupérer epci_principal & forme_juridique\n",
    "zfe_ids[\"siren\"] = zfe_ids[\"siren\"].astype(int)\n",
    "zfe_meta = zfe_meta.merge(\n",
    "    zfe_ids,\n",
    "    left_on=\"publisher_siren\",\n",
    "    right_on=\"siren\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# 5) Sauvegarde\n",
    "aires_clean.to_csv(DATA / \"aires_clean.csv\", index=False)\n",
    "voies_clean.to_csv(DATA / \"voies_clean.csv\", index=False)\n",
    "zfe_meta.to_csv(DATA / \"zfe_meta.csv\", index=False)\n",
    "\n",
    "print(\"aires_clean:\", aires_clean.shape)\n",
    "print(\"voies_clean:\", voies_clean.shape)\n",
    "print(\"zfe_meta:\", zfe_meta.shape)\n",
    "\n",
    "zfe_meta.sort_values(\"first_date_debut\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11404bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "# chemins\n",
    "HERE = Path().resolve()          # zfe-scm/\n",
    "ROOT = HERE.parent               # dossier racine\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# 1) Charger la ZFE Grenoble depuis aires.geojson\n",
    "with open(DATA / \"aires.geojson\", encoding=\"utf-8\") as f:\n",
    "    gj = json.load(f)\n",
    "\n",
    "grenoble_feats = [feat for feat in gj[\"features\"]\n",
    "                  if feat[\"publisher\"][\"zfe_id\"] == \"GRENOBLE\"]\n",
    "\n",
    "len(grenoble_feats), grenoble_feats[0][\"publisher\"][\"nom\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "# ---------- chemins ----------\n",
    "HERE = Path().resolve()          # dossier zfe-scm\n",
    "ROOT = HERE.parent               # racine du projet\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# adapte le nom du fichier si tu l'as renommé\n",
    "poll_path = DATA / \"Export Moy. journalière - 20251204215149 - 2016-02-05 00_00 - 2024-02-05 21_00.csv\"\n",
    "aires_path = DATA / \"aires.geojson\"\n",
    "\n",
    "# ---------- 1) charger le CSV et filtrer NO2 ----------\n",
    "df_raw = pd.read_csv(poll_path, sep=\";\", engine=\"python\")\n",
    "\n",
    "# colonnes de dates\n",
    "date_debut_col = [c for c in df_raw.columns if \"Date de début\" in c][0]\n",
    "df_raw[\"date\"] = pd.to_datetime(df_raw[date_debut_col])\n",
    "\n",
    "# on garde uniquement NO2\n",
    "df_no2 = df_raw[df_raw[\"Polluant\"] == \"NO2\"].copy()\n",
    "\n",
    "# on construit un dataset propre\n",
    "no2 = (\n",
    "    df_no2[[\n",
    "        \"date\",\n",
    "        \"code site\",\n",
    "        \"nom site\",\n",
    "        \"type d'implantation\",\n",
    "        \"type d'influence\",\n",
    "        \"valeur\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "    ]]\n",
    "    .rename(columns={\n",
    "        \"code site\": \"station_id\",\n",
    "        \"nom site\": \"station_name\",\n",
    "        \"type d'implantation\": \"station_env\",\n",
    "        \"type d'influence\": \"station_influence\",\n",
    "        \"valeur\": \"no2_ug_m3\",\n",
    "        \"Latitude\": \"lat\",\n",
    "        \"Longitude\": \"lon\",\n",
    "    })\n",
    "    .sort_values([\"station_id\", \"date\"])\n",
    ")\n",
    "\n",
    "no2[\"zone\"] = \"GRENOBLE\"\n",
    "\n",
    "print(\"Aperçu NO2 :\")\n",
    "display(no2.head())\n",
    "print(\"Stations :\", no2[\"station_id\"].unique())\n",
    "\n",
    "# ---------- 2) charger la ZFE Grenoble ----------\n",
    "with aires_path.open(encoding=\"utf-8\") as f:\n",
    "    gj = json.load(f)\n",
    "\n",
    "grenoble_feats = [feat for feat in gj[\"features\"]\n",
    "                  if feat[\"publisher\"][\"zfe_id\"] == \"GRENOBLE\"]\n",
    "\n",
    "# au cas où il y aurait plusieurs polygones, on les unionne\n",
    "zfe_geom = shape(grenoble_feats[0][\"geometry\"])\n",
    "\n",
    "# ---------- 3) vérifier que Les Frênes et Boulevards sont dans la ZFE ----------\n",
    "coords = (\n",
    "    no2.groupby([\"station_id\", \"station_name\"])[[\"lat\", \"lon\"]]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "for _, row in coords.iterrows():\n",
    "    pt = Point(row[\"lon\"], row[\"lat\"])\n",
    "    inside = zfe_geom.contains(pt)\n",
    "    print(f\"{row['station_id']} ({row['station_name']}) dans la ZFE GRENOBLE ? -> {inside}\")\n",
    "\n",
    "# (optionnel) sauvegarder le dataset propre pour plus tard\n",
    "out_path = DATA / \"pollution_grenoble_no2_daily_clean.csv\"\n",
    "no2.to_csv(out_path, index=False)\n",
    "print(\"✅ Dataset NO2 propre sauvegardé ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# ---------- chemins ----------\n",
    "HERE = Path().resolve()      # dossier zfe-scm\n",
    "ROOT = HERE.parent           # racine du projet\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "poll_path = DATA / \"Export Moy. journalière - 20251205011655 - 2016-02-05 00_00 - 2024-02-05 00_00.csv\"\n",
    "aires_path = DATA / \"aires.geojson\"\n",
    "\n",
    "print(\"Fichier pollution :\", poll_path.exists(), poll_path)\n",
    "\n",
    "# ---------- 1) charger le CSV et filtrer NO2 ----------\n",
    "df_raw = pd.read_csv(poll_path, sep=\";\", engine=\"python\")\n",
    "\n",
    "# repérer la colonne \"Date de début\" (il peut y avoir un caractère caché)\n",
    "date_debut_col = [c for c in df_raw.columns if \"Date de début\" in c][0]\n",
    "df_raw[\"date\"] = pd.to_datetime(df_raw[date_debut_col])\n",
    "\n",
    "# garder uniquement NO2\n",
    "df_no2 = df_raw[df_raw[\"Polluant\"] == \"NO2\"].copy()\n",
    "\n",
    "# dataset \"propre\" : colonnes utiles + renommage\n",
    "no2 = (\n",
    "    df_no2[[\n",
    "        \"date\",\n",
    "        \"code site\",\n",
    "        \"nom site\",\n",
    "        \"type d'implantation\",\n",
    "        \"type d'influence\",\n",
    "        \"valeur\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "    ]]\n",
    "    .rename(columns={\n",
    "        \"code site\": \"station_id\",\n",
    "        \"nom site\": \"station_name\",\n",
    "        \"type d'implantation\": \"station_env\",\n",
    "        \"type d'influence\": \"station_influence\",\n",
    "        \"valeur\": \"no2_ug_m3\",\n",
    "        \"Latitude\": \"lat\",\n",
    "        \"Longitude\": \"lon\",\n",
    "    })\n",
    "    .sort_values([\"station_id\", \"date\"])\n",
    ")\n",
    "\n",
    "print(\"Aperçu NO2 propre :\")\n",
    "display(no2.head())\n",
    "print(\"Stations dans le fichier :\", no2[\"station_id\"].unique())\n",
    "\n",
    "# ---------- 2) table méta des stations ----------\n",
    "stations_meta = (\n",
    "    no2.groupby([\"station_id\", \"station_name\", \"station_env\", \"station_influence\"])[[\"lat\", \"lon\"]]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Table stations_meta :\")\n",
    "display(stations_meta)\n",
    "\n",
    "# ---------- 3) charger la ZFE Grenoble ----------\n",
    "with aires_path.open(encoding=\"utf-8\") as f:\n",
    "    gj = json.load(f)\n",
    "\n",
    "grenoble_feats = [feat for feat in gj[\"features\"]\n",
    "                  if feat[\"publisher\"][\"zfe_id\"] == \"GRENOBLE\"]\n",
    "\n",
    "geoms = [shape(feat[\"geometry\"]) for feat in grenoble_feats]\n",
    "zfe_grenoble = unary_union(geoms)\n",
    "\n",
    "# ---------- 4) vérifier pour chaque station si elle est dans la ZFE ----------\n",
    "def is_in_zfe(row):\n",
    "    pt = Point(row[\"lon\"], row[\"lat\"])\n",
    "    return zfe_grenoble.contains(pt)\n",
    "\n",
    "stations_meta[\"in_zfe_grenoble\"] = stations_meta.apply(is_in_zfe, axis=1)\n",
    "\n",
    "print(\"Stations et appartenance à la ZFE Grenoble :\")\n",
    "display(stations_meta)\n",
    "\n",
    "# ---------- 5) sauvegarde des datasets propres ----------\n",
    "no2_out = DATA / \"no2_all_stations_daily_clean.csv\"\n",
    "stations_out = DATA / \"no2_stations_meta.csv\"\n",
    "\n",
    "no2.to_csv(no2_out, index=False)\n",
    "stations_meta.to_csv(stations_out, index=False)\n",
    "\n",
    "print(\"✅ NO2 daily propre sauvegardé ->\", no2_out)\n",
    "print(\"✅ Méta stations sauvegardée ->\", stations_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
